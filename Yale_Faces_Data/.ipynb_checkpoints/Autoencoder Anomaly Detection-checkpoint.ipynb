{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt  \n",
    "from PIL import Image\n",
    "from scipy.io import loadmat  \n",
    "from scipy import stats  \n",
    "from scipy.stats import multivariate_normal\n",
    "import re\n",
    "import glob\n",
    "from operator import itemgetter \n",
    "import random\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from processing_functions import * #\n",
    "\n",
    "import os\n",
    "os.chdir('../') # Go to the parent folder\n",
    "from support_functions import *\n",
    "from Autoencoder_Functions import *\n",
    "from PCA_Functions import *\n",
    "os.chdir('Yale_Faces_Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544  images are above our threshold and thus removed from the list\n"
     ]
    }
   ],
   "source": [
    "label_1_folder = [9,21]      # Images in this folder will be labeled as anomaly\n",
    "target_folders = range(1,21) # Read these folders\n",
    "data_path = \"CroppedYale/\"   # Source folder\n",
    "\n",
    "# Read image matrix (n*m), labels (vector of m), and image size\n",
    "imgs, labels, height, width = get_data(label_1_folder,target_folders,data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Saved Deep Net Models\n",
    "#### Load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/ML/Anomaly_Detection/Autoencoder_Functions.py:22: VisibleDeprecationWarning: boolean index did not match indexed array along dimension 0; dimension is 190800 but corresponding boolean dimension is 1\n",
      "  data_normal = data[labels == 0]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 504 is out of bounds for axis 1 with size 504",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-bcf9388b86b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mimgs_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlabels_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mautoencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_autoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_rep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_layers_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder_layers_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Load an existing model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ivan/ML/Anomaly_Detection/Autoencoder_Functions.py\u001b[0m in \u001b[0;36mtrain_autoencoder\u001b[0;34m(data, labels, encoder_layers_size, decoder_layers_size, save_model)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Prepare the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Select only the Normal Image Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mdata_normal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_normal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Split the images and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 504 is out of bounds for axis 1 with size 504"
     ]
    }
   ],
   "source": [
    "# Specify the model config\n",
    "encoder_layers_size = [128, 64, 32]\n",
    "decoder_layers_size = [64, 128]\n",
    "\n",
    "# Load an existing model\n",
    "img_size = imgs.shape[1]\n",
    "autoencoder,encoder = compile_autoencoder(imgs, img_size,encoder_layers_size,decoder_layers_size) # Generate and Compile a Deep Autoencoder\n",
    "autoencoder = load_model('model_autoencoder_deep3.h5') # Load the saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second, set the weights of the encoder model with the first half of autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights_encoder = autoencoder.get_weights()[0:2] # The first half of the autoencoder model is an encoder model\n",
    "encoder.set_weights(weights_encoder) # Set weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(encoder.summary())\n",
    "print(\"\\n The output shape of the encoder model: \")\n",
    "print(encoder.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Reconstructed Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "imgs_reconstructed = reconstruct_with_autoencoder(autoencoder,imgs,visual =True,height = height, width = width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the Anomaly Detection \n",
    "### Split the data into the Training and Testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the images and labels\n",
    "# By default: 80% in training and 20% in testing\n",
    "train_ind, test_ind = split_training(labels)\n",
    "x_train = imgs[train_ind,:]\n",
    "x_test = imgs[test_ind,:]\n",
    "x_train_reconstructed = imgs_reconstructed[train_ind,:]\n",
    "x_test_reconstructed = imgs_reconstructed[test_ind,:]\n",
    "labels_train = labels[train_ind]\n",
    "labels_test = labels[test_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Anomaly Detection with the Reconstruction Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_test_with_reconstruction_error(x_train, x_train_reconstructed, x_test, x_test_reconstructed, labels_train, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Detector with the Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the euclidean distance between the reconstructed dataset and the original ()\n",
    "dist_test = find_euclidean_distance(reconst_test,np.transpose(x_test))\n",
    "\n",
    "# Sort the Images and Labels based on the Probability\n",
    "rank_test = np.argsort(-dist_test) # Sort from the Largest to the Smallest\n",
    "dist_test_ranked = dist_test[rank_test] # Sort the distance\n",
    "labels_test_ranked = labels_test[rank_test] # Rank Labels\n",
    "\n",
    "# Give Predictions\n",
    "preds = np.zeros(labels_test.shape) # Initialization\n",
    "preds[dist_test_ranked > threshold_reconst_error] = 1\n",
    "\n",
    "# Evaluate the Detector with Testing Data\n",
    "eval_with_test(preds, labels_test_ranked, k = k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Anomaly Detection with the Gaussian Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Encode the images in the Training Set and the Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pass the training data into the decoder model\n",
    "encoded_train = encoder.predict(x_train)\n",
    "encoded_test = encoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the Distribution of Data Points in the encoded matrix\n",
    "To observe the scale and the Gaussian property "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_matrix_data(encoded_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit a Gaussian Distribution Model with the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get Gaussian Distribution Model for the Validation Data\n",
    "# Note: fit_multivariate_gaussian() is my own coded function\n",
    "dist_train = fit_multivariate_gaussian(encoded_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the Distribution of the Anomaly vs. Normal Set through Scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get Probability of being Anomaly vs. being Normal\n",
    "p_train = dist_train.pdf(encoded_train)   # Probability of Being Normal\n",
    "# Plot the Probability with labels\n",
    "plot_scatter_with_labels(p_train, labels_train,'Gaussian Probability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the Anomaly Detector\n",
    "threshold_gaussian  = select_threshold_probability(p_train, labels_train,k,to_print = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply to the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the euclidean distance between the reconstructed dataset and the original ()\n",
    "p_test = dist_train.pdf(encoded_test)   # Probability of Being Normal\n",
    "\n",
    "# Sort the Images and Labels based on the Probability\n",
    "rank_test = np.argsort(p_test) # Sort from the Smallest to the Largest\n",
    "p_test_ranked = p_test[rank_test] # Sort the distance\n",
    "labels_test_ranked = labels_test[rank_test] # Rank Labels\n",
    "\n",
    "# Give Predictions\n",
    "preds = np.zeros(labels_test.shape) # Initialization\n",
    "preds[p_test_ranked < threshold_gaussian] = 1 # If the probability is smaller than the threshold, marked as anomaly\n",
    "\n",
    "# Evaluate the Detector with Testing Data\n",
    "eval_with_test(preds, labels_test_ranked, k = k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
