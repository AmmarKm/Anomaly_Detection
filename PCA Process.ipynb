{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Source: http://vision.ucsd.edu/~iskwak/ExtYaleDatabase/ExtYaleB.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt  \n",
    "from PIL import Image\n",
    "from scipy.io import loadmat  \n",
    "from scipy import stats  \n",
    "from scipy.stats import multivariate_normal\n",
    "import re\n",
    "import glob\n",
    "from operator import itemgetter \n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_images(data_path,target_folders,label_1_folder,reduce_height = 24,reduce_width = 21):\n",
    "    \"\"\"\n",
    "    This function reads in all images inside the specified folders, and label the images based on label_1_folder\n",
    "    data_path: the path of the folder where all the image folders reside in\n",
    "    target_folders: the target_folders to be read from\n",
    "    label_1_folder: images in the specified folders will be labeled with 1\n",
    "    \"\"\"\n",
    "    # label_1_folder = [9,21]\n",
    "    folder_paths = glob.glob(data_path + \"*\")\n",
    "    images = [] # Initialize a list to record images\n",
    "    labels = [] # Initialize a list to record labels\n",
    "    for folder_path in folder_paths:\n",
    "        index = int(folder_path[-2:]) # Get the index embeded in the folder path\n",
    "        if index in target_folders:\n",
    "            # Assign labels\n",
    "            if index in label_1_folder:\n",
    "                label =1\n",
    "            else:\n",
    "                label = 0\n",
    "\n",
    "            # Read in images and corresponding labels\n",
    "            img_paths = glob.glob(folder_path + \"/*.pgm\")\n",
    "            for img_path in img_paths: \n",
    "                if img_path.find(\"Ambient\")>0:\n",
    "                    img_paths.remove(img_path) # We do not want the \"Ambient\" image because it is a profile picture\n",
    "                else:\n",
    "                    # img = plt.imread(img_path) # Used to read image without resizing\n",
    "                    img_raw = Image.open(img_path) # Used when we need to resize the image (downsize in this case)\n",
    "                    img_reduce = img_raw.resize((reduce_width, reduce_height), Image.BILINEAR) # Resize the image\n",
    "                    img = np.array(img_reduce) # This step is necessary if we use Image.open()\n",
    "                    images.append(img)\n",
    "                    labels.append(label)\n",
    "    return images,labels\n",
    "\n",
    "\n",
    "def dark_pixel_curve(images,light_threshold = 20):\n",
    "    \"\"\"\n",
    "    Images are taken at different lighting conditions; thus some of the photos are dark. In order to avoid \n",
    "    the impact of the bad lighting conditions, we need to remove photos with large number of dark pixels. \n",
    "    This curve shows us the number of images to be removed at different thresholds (total number of pixels \n",
    "    that are below 20 in one image). It can help us select an appropriate threshold. \n",
    "    \"\"\"\n",
    "    height, width = images[0].shape # Get the dimension of one image\n",
    "    images_num = len(images)\n",
    "    thresh_list = range(100,height*width,100) # Threshold levels to be tested: from 100 to the total pixels\n",
    "    remove_list = []\n",
    "    for dark_pixel_threshold in thresh_list:\n",
    "        remove_count = 0\n",
    "        for i in range(0,images_num):\n",
    "            if sum(sum(images[i] < light_threshold)) > dark_pixel_threshold:\n",
    "                remove_count = remove_count + 1\n",
    "        remove_list.append(remove_count)\n",
    "    \n",
    "    plt.plot(thresh_list,remove_list)\n",
    "    plt.xlabel(\"Number of dark pixels in an image\")\n",
    "    plt.ylabel(\"Number of images to be removed from the list\")\n",
    "    plt.title(\"Select the right threshold level\")\n",
    "    \n",
    "def remove_dark_img(imgs,labels,dark_pixel_threshold,light_threshold = 20):\n",
    "    \"\"\"\n",
    "    This function remove images that have more dark pixels (<20) than our threshold\n",
    "    \"\"\"\n",
    "    remove_count = 0\n",
    "    imgs_num = len(imgs)\n",
    "    for i in range(imgs_num-1,0-1,-1):\n",
    "        if sum(sum(imgs[i] < light_threshold)) > dark_pixel_threshold:\n",
    "            del imgs[i]\n",
    "            del labels[i]\n",
    "            remove_count = remove_count + 1\n",
    "    print (remove_count,' images are above our threshold and thus removed from the list')\n",
    "    return imgs,labels,remove_count\n",
    "\n",
    "def plot_images(imgs,labels):\n",
    "    \"\"\"\n",
    "    Plot 25 images selected randomly\n",
    "    \"\"\"\n",
    "    ind = np.random.permutation(len(imgs))\n",
    "\n",
    "    # Create figure with 5x5 sub-plots.\n",
    "    fig, axes = plt.subplots(5, 5,figsize=(15,15))\n",
    "    fig.subplots_adjust(hspace=0.1, wspace=0.01)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat): \n",
    "        ax.imshow(imgs[ind[i]], plt.cm.gray)\n",
    "        xlabel = \"Anomaly: {0}\".format(labels[ind[i]])\n",
    "        # Show the classes as the label on the x-axis.\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def show_anomaly_images(images,labels):\n",
    "    \"\"\"\n",
    "    This function randomly show 9 images with label 1\n",
    "    \"\"\"\n",
    "    anomaly_label_index = np.asarray(np.where(labels)).reshape(-1) # Get the indice of anomaly\n",
    "    anomaly_image = [images[i] for i in anomaly_label_index] # Extract the images labeled as anomaly\n",
    "    anomaly_label = [labels[i] for i in anomaly_label_index] # Extract the images labeled as anomaly\n",
    "    plot_images(anomaly_image,anomaly_label) # Show 9 images randomly\n",
    "    \n",
    "def plot_eigenfaces(pca_matrix,height, width):\n",
    "    \"\"\"\n",
    "    This function plot the eigenfaces based on the given PCA Matrix\n",
    "    \"\"\"\n",
    "    n_eigen = pca_matrix.shape[1]\n",
    "    # Define the layout of the plots\n",
    "    n_row = 4\n",
    "    n_col = n_eigen//n_row \n",
    "\n",
    "    # Create figure with 3x3 sub-plots.\n",
    "    fig, axes = plt.subplots(n_row, n_col,figsize=(15,15))\n",
    "    fig.subplots_adjust(hspace=0.1, wspace=0.01)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat): \n",
    "        ax.imshow(pca_matrix[:,i].reshape(height, width), plt.cm.gray)\n",
    "        xlabel = \"Eigenface: {0}\".format(i+1)\n",
    "        # Show the classes as the label on the x-axis.\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "def mean_shift(components):\n",
    "    \"\"\"\n",
    "    This function applies mean shift to each component in the component matrix\n",
    "    The input components is a n*m matrix. Each row correspons to one component.\n",
    "    It is important to return the components' mean vector: we will need it in PCA reconstruction\n",
    "    \"\"\"\n",
    "    component_mean = np.mean(components,axis = 1)\n",
    "    shifted_components = (components.T - component_mean).T # Necessary to take transpose twice here\n",
    "    return shifted_components, component_mean\n",
    "\n",
    "def check_eigen(eigen_value, eigen_vector,cov_matrix):\n",
    "    \"\"\"\n",
    "    This function check the correctness of eigenvector & eigenvalue through the equation\n",
    "    cov_matrix * eigen_vector = eigen_value * eigen_vector\n",
    "    \"\"\"\n",
    "    for i in range(len(eigen_value)): \n",
    "        n = cov_matrix.shape[1]\n",
    "        eigv = eigen_vector[:,i].reshape(1,n).T \n",
    "        np.testing.assert_array_almost_equal(cov_matrix.dot(eigv), eigen_value[i] * eigv, decimal=6, err_msg='', verbose=True)\n",
    "        \n",
    "def plot_compare_after_reconst(img_matrix_reconst,imgs_matrix,height,width):\n",
    "    \"\"\"\n",
    "    This function compares the images reconstructed after PCA with their original one.\n",
    "    The shape of both image matrice in the input is n*m, where n is the number of components, \n",
    "    and m is the number of images.\n",
    "    \"\"\"\n",
    "    # Permutate through the image index\n",
    "    ind = np.random.permutation(imgs_matrix.shape[1])\n",
    "\n",
    "    # Create figure with multiple sub-plots.\n",
    "    fig, axes = plt.subplots(4, 4,figsize=(15,15))\n",
    "    fig.subplots_adjust(hspace=0.1, wspace=0.01)\n",
    "\n",
    "    # Initialize the counter of images\n",
    "    image_count = 0 \n",
    "\n",
    "    for i, ax in enumerate(axes.flat): \n",
    "        if i % 2 == 0:\n",
    "            image_count += 1\n",
    "            ax.imshow(imgs_matrix[:,ind[i]].reshape(height,width), plt.cm.gray)\n",
    "            xlabel = \"Example {0}: Original Image\".format(image_count)\n",
    "        else:\n",
    "            ax.imshow(img_matrix_reconst[:,ind[i-1]].reshape(height,width), plt.cm.gray)\n",
    "            xlabel = \"Example {0}: Reconstructed from PCA\".format(image_count)\n",
    "        # Show the classes as the label on the x-axis.\n",
    "        ax.set_xlabel(xlabel)\n",
    "\n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def split_training(labels,ratio):\n",
    "    \"\"\"\n",
    "    This function Split the Data into the Training and Validation Set. \n",
    "    Its output is the indice of images to be assigned to the Training/Validation Set. \n",
    "    The input \"labels\" is a hvector\n",
    "    The ratio is a number between [0,1] that represents the percentage of images to be assigned to the training set\n",
    "    \"\"\"\n",
    "    ratio = 0.8 # The percentage of images to be splitted into the training set\n",
    "    m = len(labels)\n",
    "    training_size = int(m*ratio)\n",
    "    while 1:\n",
    "        ind = np.random.permutation(m) # Permutate to generate random indice within m\n",
    "        train_ind = ind[:training_size-1]\n",
    "        test_ind = ind[training_size:]\n",
    "        # if (sum(itemgetter(*train_ind)(labels)) > 0 and sum(itemgetter(*test_ind)(labels)) > 0):\n",
    "        if (sum(labels[train_ind]) > 0 and sum(labels[test_ind]) > 0):\n",
    "            break\n",
    "    return train_ind, test_ind\n",
    "\n",
    "def estimate_gaussian(X):\n",
    "    \"\"\"\n",
    "    Compute the parameters of the Gaussian Distribution\n",
    "    Note: X is given in the shape of m*k, where k is the number of (reduced) dimensions, and m is the number of images\n",
    "    \"\"\"\n",
    "    mu =np.mean(X,axis=0)\n",
    "    cov = np.cov(X,rowvar=0)\n",
    "\n",
    "    return mu, cov\n",
    "\n",
    "def fit_multivariate_gaussian(data):\n",
    "    \"\"\"\n",
    "    This function is used to compute the mu and cov based on the given data, and fit a multivariate gaussian dist\n",
    "    This data is given as a m*k matrix, where m represents the number of samples, and k represents the number of dimensions\n",
    "    \"\"\"\n",
    "    mu, cov = estimate_gaussian(data)\n",
    "    dist = multivariate_normal(mean = mu, cov = cov)\n",
    "    return dist\n",
    "\n",
    "def select_threshold(pval, yval, p_anomaly_switch = 0):  \n",
    "    \"\"\"\n",
    "    This function finds the best threshold value to detect the anomaly given the PDF values and True label Values\n",
    "    pval: Probability based on the Multivariate Gaussian Distribution\n",
    "    yval: True label value\n",
    "    p_anomaly_switch: 1 if the given pval is the probability of being an anomaly, otherwise that is the probability\n",
    "    of not being an anmaly. By default it is 0\n",
    "    \"\"\"\n",
    "    best_epsilon = 0\n",
    "    best_f1 = 0\n",
    "    best_tp = 0\n",
    "    best_fp = 0\n",
    "    best_fn = 0\n",
    "\n",
    "    step = (pval.max() - pval.min()) / 1000\n",
    "\n",
    "    for epsilon in np.arange(pval.min(), pval.max(), step):\n",
    "        # If the given p is the probability of being an anomaly, we need to find the threshold so that\n",
    "        # if p > threshold, it will be an anomaly\n",
    "        # IF the given p is hte probability of not being an anomaly, we need to find the threshold so that \n",
    "        # if p < threshold, it will be an anomaly\n",
    "        if p_anomaly_switch == 1:\n",
    "            preds = pval > epsilon\n",
    "        else:\n",
    "            preds = pval < epsilon\n",
    "\n",
    "        tp,tn,fp,fn,f1 = eval_prediction(preds,yval)\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_epsilon = epsilon\n",
    "            best_tp = tp\n",
    "            best_fp = fp\n",
    "            best_fn = fn\n",
    "    #return best_epsilon, best_f1, best_tp, best_fp, best_fn\n",
    "    return best_epsilon\n",
    "\n",
    "def eval_prediction(pred,yval,rate = False):\n",
    "    \"\"\"\n",
    "    Function to evaluate the correctness of the predictions with multiple metrics\n",
    "    If rate = True, we will return all the metrics in rate (%) format (except f1)\n",
    "    \"\"\"\n",
    "    true_positive = np.sum(np.logical_and(pred == 1, yval == 1)).astype(float) # True Positive\n",
    "    true_negative = np.sum(np.logical_and(pred == 0, yval == 0)).astype(float) # True Negative\n",
    "    false_positive = np.sum(np.logical_and(pred == 1, yval == 0)).astype(float) # False Positive\n",
    "    false_negative = np.sum(np.logical_and(pred == 0, yval == 1)).astype(float) # False Negative\n",
    "\n",
    "    precision = true_positive / max(1,true_positive + false_positive)\n",
    "    recall = true_positive / max(1,true_positive + false_negative)\n",
    "    f1 = (2 * precision * recall) / max(1,precision + recall)\n",
    "    # A more direct version of f1 is f1 = 2*tp/(2*tp+fn+fp)\n",
    "    \n",
    "    if rate:\n",
    "        n_p = sum(yval == 1)     # Number of Positive\n",
    "        n_n = yval.shape[0] - n_p # Number of Negative\n",
    "        tpr = true_positive/n_p\n",
    "        tnr = true_negative/n_n\n",
    "        fpr = false_positive/n_n\n",
    "        fnr = false_negative/n_p\n",
    "        return tpr,tnr,fpr,fnr,f1\n",
    "    else:\n",
    "        return true_positive,true_negative,false_positive,false_negative,f1\n",
    "    \n",
    "def compute_pca_matrix(data, n_components):\n",
    "    \"\"\"\n",
    "    This function compute the pca matrix with the given data\n",
    "    The data should be given in the matrix form n*m, where n is the number of dimensions, and m is the number of samples\n",
    "    The data should have been processed with mean-shift\n",
    "    n_components: Number of components to be kept after PCA\n",
    "    \"\"\"\n",
    "    # Compute the Covariance Matrix\n",
    "    cov_matrix = np.cov(data)\n",
    "\n",
    "    # Compute the eigen value and eigen vectors\n",
    "    eigen_value, eigen_vector = np.linalg.eig(cov_matrix)\n",
    "\n",
    "    # Sort the eigenvectors by decreasing eigenvalues\n",
    "    # First make a list of (eigenvalue, eigenvector) tuples \n",
    "    eig_pairs = [(np.abs(eigen_value[i]), eigen_vector[:,i]) for i in range(len(eigen_value))] \n",
    "    # Sort the (eigenvalue, eigenvector) tuples from high to low \n",
    "    eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    # Convert the sorted eigen vector list to matrix form\n",
    "    eigen_vector_sorted = np.zeros((height*width,n_components))\n",
    "    for i in range(0,n_components):\n",
    "        eigen_vector_sorted[:,i] = eig_pairs[i][1]\n",
    "\n",
    "    # Cut the sorted eigenvectors by columns to get the transformational matrix for PCA\n",
    "    pca_matrix = eigen_vector_sorted[:,:n_components]\n",
    "\n",
    "    return pca_matrix\n",
    "\n",
    "def find_euclidean_distance(matrix1,matrix2):\n",
    "    \"\"\"\n",
    "    This function find the Euclidean Distance between two Matric\n",
    "    The distance is between the same columns of two matric\n",
    "    \"\"\"\n",
    "    dist = np.linalg.norm(matrix1 - matrix2,axis = 0) # By specifying axis = 0, we find the distance between columns\n",
    "    return dist\n",
    "\n",
    "def select_threshold_distance(edistance, yval):  \n",
    "    \"\"\"\n",
    "    This function finds the best threshold value to detect the anomaly given the PDF values and True label Values\n",
    "    edistance: euclidean distance \n",
    "    yval: True label value\n",
    "    \"\"\"\n",
    "    best_epsilon = 0\n",
    "    best_f1 = 0\n",
    "    best_tpr = 0\n",
    "    best_tnr = 0\n",
    "    best_fpr = 0\n",
    "    best_fnr = 0\n",
    "\n",
    "    step = (edistance.max() - edistance.min()) / 1000\n",
    "\n",
    "    for epsilon in np.arange(edistance.min(), edistance.max(), step):\n",
    "        preds = edistance > epsilon\n",
    "        \n",
    "        tpr,tnr,fpr,fnr,f1 = eval_prediction(preds,yval,rate = True)\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_epsilon = epsilon\n",
    "            best_tpr = tpr\n",
    "            best_tnr = tnr\n",
    "            best_fpr = fpr\n",
    "            best_fnr = fnr\n",
    "    return best_epsilon,best_tpr,best_tnr,best_fpr,best_fnr,best_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Images and Remove Over-shadowed Ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the images to be read and the corresponding labels\n",
    "label_1_folder = [9,21]\n",
    "target_folders = range(1,22)\n",
    "data_path = \"Yale Face B/CroppedYale/\"\n",
    "\n",
    "# We also need to reduce the size of the image for the convenience of computation\n",
    "reduce_height = 24\n",
    "reduce_width = 21\n",
    "\n",
    "# Read the images and reduce the size\n",
    "images,labels = read_images(data_path,target_folders,label_1_folder,reduce_height,reduce_width)\n",
    "\n",
    "# Number of labels as \"Anomaly\" and Total Number of Labels\n",
    "sum(labels),len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# To evaluate the threshold of the dark pixels\n",
    "# dark_pixel_curve(images)\n",
    "\n",
    "imgs = images[:] # Create a copy\n",
    "# Eliminate the images and labels whose number of dark pixels are above the threshold\n",
    "# The threshold is determined based on the dark_pixel_curve() function above\n",
    "imgs,labels,remove_count = remove_dark_img(imgs,labels,180) \n",
    "\n",
    "plot_images(imgs,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Randomly select and show anomalous images\n",
    "show_anomaly_images(imgs,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Apply PCA for Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the number of Principal Components we want to keep from the image\n",
    "n_components = 20\n",
    "\n",
    "# Find the dimension of one image\n",
    "height, width = imgs[0].shape\n",
    "num_imgs = len(imgs)\n",
    "height, width,num_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the Images List to a 2-Dimensional Matrix\n",
    "Convert each image from 2D to 1D array, and each column of the new matrix will be one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the matrix to store the entire image list\n",
    "imgs_matrix = np.zeros((height*width,num_imgs)) \n",
    "\n",
    "# Iterate through each image, convert it into an array, and add to the imgs_matrix as a column\n",
    "for i in range(0,len(imgs)):\n",
    "    imgs_matrix[:,i] = imgs[i].reshape(height*width)\n",
    "imgs_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a Mean-Shift\n",
    "We applied the Mean-Shift on each component before computing the Covariance Matrix. It is important to save the vector of the components' mean: we will use it when we reconstruct the data after we applied PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_matrix_shifted, component_mean = mean_shift(imgs_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Covariance Matrix of the Image Matrix\n",
    "The Covariance Matrix should be a symmetric square matrix with shape $n*n$, where $n$ is the row number of the Image Matrix, or **the number of the components**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cov_matrix = np.cov(img_matrix_shifted)\n",
    "cov_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the eigenvectors and the corresponding eigenvalues based on the Covariance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the eigen value and eigen vectors\n",
    "eigen_value, eigen_vector = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Sort the eigenvectors by decreasing eigenvalues\n",
    "# First make a list of (eigenvalue, eigenvector) tuples \n",
    "eig_pairs = [(np.abs(eigen_value[i]), eigen_vector[:,i]) for i in range(len(eigen_value))] \n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low \n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# Convert the sorted eigen vector list to matrix form\n",
    "eigen_vector_sorted = np.zeros((height*width,n_components))\n",
    "for i in range(0,n_components):\n",
    "    eigen_vector_sorted[:,i] = eig_pairs[i][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the PCA Matrix and Visualize the Eigenfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cut the sorted eigenvectors by columns to get the transformational matrix for PCA\n",
    "pca_matrix = eigen_vector_sorted[:,:n_components]\n",
    "pca_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize the eigenfaces with the pca matrix\n",
    "plot_eigenfaces(pca_matrix,height, width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct the Face Images after PCA\n",
    "First we applies PCA on the image data matrix to downsize its dimensions. Then we reconstruct the image matrix through the PCA matrix and add the mean of each component back. Finally we plot several examples to compare the original face images with the reconstructed face images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute the transformed image\n",
    "# Shape of pca_matrix: n * k\n",
    "# Shape of imgs_matrix: n * m\n",
    "# Shape of the transformed face image matrix: k * m\n",
    "img_pca_tranf = pca_matrix.T.dot(img_matrix_shifted)\n",
    "\n",
    "# Reconstruct through PCA Matrix and Mean Vector\n",
    "# Shape of the reconstructed face image matrix: n * m\n",
    "# component_mean is a vector and we need to convert it to a one-column matrix for the addition\n",
    "img_matrix_reconst = pca_matrix.dot(img_pca_tranf) + component_mean.reshape(height*width,1)\n",
    "\n",
    "# Plot the original images and their reconstructed version for comparison\n",
    "plot_compare_after_reconst(img_matrix_reconst,imgs_matrix,height,width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection\n",
    "We will use two methods of the Anomaly Detection. \n",
    "1. We will use the probability based on the Multivariate Gaussian Distribution.\n",
    "2. We will use the Reconstruction Error Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data into the Training and Validation Set\n",
    "With the function, we get the indice of images to be assigned to the training and the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vectorize the labels list\n",
    "labels_vector = np.hstack(labels) # Easier to get multiple items from a vector than from a list\n",
    "\n",
    "# Split the images and labels\n",
    "ratio = 0.8 # 80% of the images will be splitted to the training set\n",
    "train_ind, test_ind = split_training(labels_vector,ratio)\n",
    "\n",
    "imgs_train = img_pca_tranf[:,train_ind]\n",
    "imgs_test = img_pca_tranf[:,test_ind]\n",
    "labels_train = labels_vector[train_ind]\n",
    "labels_test = labels_vector[test_ind]\n",
    "\n",
    "imgs_train.shape, imgs_test.shape, labels_train.shape,labels_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Multivariate Gaussian Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We need to separete the data to get different Gaussian Distribution Model\n",
    "imgs_train_anomaly = imgs_train[:,labels_train == 1]\n",
    "imgs_train_normal = imgs_train[:,labels_train == 0]\n",
    "\n",
    "# Get Gaussian Distribution Model from the Anomaly Data and Normal Data Separately\n",
    "# Note: fit_multivariate_gaussian() is my own coded function\n",
    "dist_anomaly = fit_multivariate_gaussian(imgs_train_anomaly.T)\n",
    "dist_normal = fit_multivariate_gaussian(imgs_train_normal.T)\n",
    "\n",
    "# Get Probability of being Anomaly vs. being Normal\n",
    "p_train_anomaly = dist_anomaly.pdf(imgs_train.T) # Probability of Being Abnormal\n",
    "p_train_normal = dist_normal.pdf(imgs_train.T)   # Probability of Being Normal\n",
    "\n",
    "# Train the Anomaly Detector\n",
    "threshold_anomaly = select_threshold(p_train_anomaly, labels_train, p_anomaly_switch = 1)\n",
    "threshold_normal  = select_threshold(p_train_normal, labels_train, p_anomaly_switch = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Case 1: Predict with only the threshold at the probability of being anomaly\n",
    "preds1 = p_train_anomaly > threshold_anomaly\n",
    "tpr,tnr,fpr,fnr,f1 = eval_prediction(preds1,labels_train,rate = True)\n",
    "tpr,tnr,fpr,fnr,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Case 2: Predict with only the threshold at the probability of being normal\n",
    "preds2 = p_train_normal < threshold_normal\n",
    "tpr,tnr,fpr,fnr,f1 = eval_prediction(preds2,labels_train,rate = True)\n",
    "tpr,tnr,fpr,fnr,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Case 3: Predict as Anomaly if one of the detector claims as Anomaly\n",
    "np.logical_or(p_train_anomaly > threshold_anomaly, p_train_normal < 0)\n",
    "# preds3 = (p_train_anomaly < threshold_anomaly)*(p_train_normal > threshold_normal)\n",
    "preds3 = np.logical_or(p_train_anomaly > threshold_anomaly, p_train_normal < threshold_normal)\n",
    "tpr,tnr,fpr,fnr,f1 = eval_prediction(preds3,labels_train,rate = True)\n",
    "tpr,tnr,fpr,fnr,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Case 4: Predict as Anomaly if both of the detectors claim as Anomaly\n",
    "preds4 = np.logical_and(p_train_anomaly > threshold_anomaly, p_train_normal < threshold_normal)\n",
    "tpr,tnr,fpr,fnr,f1 = eval_prediction(preds4,labels_train,rate = True)\n",
    "tpr,tnr,fpr,fnr,f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the examination above, we can find the detector of being anomaly alone perform much better than the detector of being abnormal.\n",
    "\n",
    "#### Apply on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_test_anomaly = dist_anomaly.pdf(imgs_test.T)   # Probability of Being Normal\n",
    "preds_test = p_test_anomaly > threshold_anomaly\n",
    "tpr,tnr,fpr,fnr,f1 = eval_prediction(preds_test,labels_test,rate = True)\n",
    "tpr,tnr,fpr,fnr,f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: PCA Reconstruction Error\n",
    "The method will measure the Euclidean distance between each data point on the image and its new point after reconstruction from the PCA, whcih was generated sololy with the Normal Data. The basis of this method is that the Anomalies will give large values of distance. I will use the training data to find the optimal threshold to identify the Anomalies. \n",
    "\n",
    "#### Generate a new PCA sololy with the Normal Data\n",
    "Since we need to re-run a new PCA, we need to go back to the place where we took a Mean Shift on the Image Matrix and start over again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Isolate the Normal Data from the Training Set\n",
    "# Note: we use the same index but on the img_matrix_shifted\n",
    "img_matrix_shifted_train = img_matrix_shifted[:,train_ind]\n",
    "img_matrix_shifted_train_normal = img_matrix_shifted_train[:,labels_train == 0]\n",
    "\n",
    "pca_matrix_normal = compute_pca_matrix(img_matrix_shifted_train_normal, n_components)\n",
    "\n",
    "# Visualize the new eigenfaces\n",
    "#plot_eigenfaces(pca_matrix_normal,height, width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconstruct the Data with the new PCA Matrix\n",
    "Use the PCA on both the Normal and Anomalous Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_pca_tranf_new = pca_matrix_normal.T.dot(img_matrix_shifted) # Apply on all the data\n",
    "\n",
    "# Reconstruct through PCA Matrix and Mean Vector\n",
    "img_matrix_reconst_new = pca_matrix_normal.dot(img_pca_tranf_new) + component_mean.reshape(height*width,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the Optimal Threshold based on the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the euclidean distance between the reconstructed dataset and the original ()\n",
    "dist = find_euclidean_distance(img_matrix_reconst_new,imgs_matrix)\n",
    "np.min(dist),np.mean(dist),np.max(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dist_train= dist[train_ind]\n",
    "threshold2, tpr,tnr,fpr,fnr,f1 = select_threshold_distance(dist_train, labels_train)\n",
    "tpr,tnr,fpr,fnr,f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dist_test= dist[test_ind]\n",
    "preds_test2 = dist_test > threshold2\n",
    "tpr,tnr,fpr,fnr,f1 = eval_prediction(preds_test2,labels_test,rate = True)\n",
    "tpr,tnr,fpr,fnr,f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
