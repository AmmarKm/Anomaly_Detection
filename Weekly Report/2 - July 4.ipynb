{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekly Report - July 4\n",
    "## Summary of Contents\n",
    "- Goals\n",
    "- Results & Findings\n",
    "- Plan of the Next Week\n",
    "- Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "My goals of the week include:\n",
    "- Implement more metrices to evaluate anomaly detectors\n",
    "- Anlayze and address the fallacy of the Gaussian-based anomaly detectors\n",
    "- Assess different configuration of Deep Autoencoders\n",
    "- Apply Anomaly Detection on Synthetic Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Results & Findings\n",
    "### 1. Anomaly Detection on Synthetic Datasets\n",
    "I created 4 synthetic binary vector datasets. Here is a brief introduction of them:\n",
    "\n",
    "#### Characteristics of the 4 dataset\n",
    "|Dataset|#1|#2|#3|#4|\n",
    "|---|---|---|---|---|\n",
    "|Sample Size|85,322|100,000|100,000|85,674|\n",
    "|No. Dimensions|16|16|16|16|\n",
    "|No of gaussian distribution used to generate the dataset|1|1|3|2|\n",
    "|%Anomaly|11.8%|13.1%|10.0%|11.2%|\n",
    "|Anomaly|the total number of '1's in the vector is less than the threshold (4)|The sum of the right (`n-1`) digits is even **AND** the leftmost digit is even (1)|Generated with two different distribution other than the normal one|the total number of '1's in the vector is less than the threshold (4)|\n",
    "\n",
    "#### Basic Method to Generate Data - Achieve Correlation and Binary\n",
    "\n",
    "- First, I generated a dataset with a multivariate distribution with a random mean vector and a covariance matrix\n",
    "- Second, I convert the dataset to binary by setting all the numbers that is larger or equal to 0.5 to 1, and others to 0. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detector Evaluation - Reconstruction Error with PCA\n",
    "Work well in the dataset 3\n",
    "\n",
    "|Dataset|#1|#2|#3|#4|\n",
    "|---|---|---|---|---|\n",
    "|Precision|16.4%|18.0%|63.7%|16.2%|\n",
    "|Recall|100%|81.8%|50.4%|81.9%|\n",
    "|R-Precision|18.6%|17.9%|63.7%|9.3%|\n",
    "|Precision@50|12.0%|26.0%|100%|10.0%|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detector Evaluation - Gaussian Models with PCA\n",
    "|Dataset|#1|#2|#3|#4|\n",
    "|---|---|---|---|---|\n",
    "|Precision|12.9%||||\n",
    "|Recall|100%||||\n",
    "|R-Precision|22.9%||||\n",
    "|Precision@50|0||||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detector Evaluation - Reconstruction Error with Autoencoder\n",
    "|Dataset|#1|#2|#3|#4|\n",
    "|---|---|---|---|---|\n",
    "|Precision|20.2%||||\n",
    "|Recall|82.9%||||\n",
    "|R-Precision|14.8%||||\n",
    "|Precision@50|4.0%||||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detector Evaluation - Gaussian Models with Autoencoder\n",
    "Work amazingly well in the dataset 1\n",
    "\n",
    "|Dataset|#1|#2|#3|#4|\n",
    "|---|---|---|---|---|\n",
    "|Precision|71.1%||||\n",
    "|Recall|94.5%%||||\n",
    "|R-Precision|76.7%%||||\n",
    "|Precision@50|100.0%||||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendices\n",
    "### Appendix 1: code to generate the Synthetic Dataset 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle\n",
    "\n",
    "# Generate 100K numbers, each of which has 16 digits\n",
    "# Anomaly: number of 1s is larger than 2\n",
    "\n",
    "# Set Parameteres\n",
    "n_dimensions = 16\n",
    "n_samples = 10**5\n",
    "data1_ratio = 0.5 # Dataset 1\n",
    "data2_ratio = 0.5 # Dataset 2\n",
    "Anomaly_Threshold = 4 # Anomaly if total # 1s is less than the threshold\n",
    "\n",
    "def generate_random_mg_data(n_dimensions, n_samples):\n",
    "    \"\"\"\n",
    "    Generate a random dataset \n",
    "    \"\"\"\n",
    "    mu = np.random.rand(n_dimensions) # Random vector for mean\n",
    "    cov = np.random.rand(n_dimensions,n_dimensions) # Random matrix for covaraince\n",
    "    data_mg = np.random.multivariate_normal(mu, cov, size=n_samples) # Generate a random matrix with multivariate normal distribution\n",
    "    data_bi = data_mg >= 0.5 # Convert to binary - True if the data is larger than 0.5; otherwise 0\n",
    "    data = data_bi*1 # Convert True/False to 1/0\n",
    "    return data\n",
    "\n",
    "def generate_random_data_2md(n_dimensions, data1_size, data2_size):\n",
    "    \"\"\"\n",
    "    Generate a random data set with two multivate gaussian distribution\n",
    "    \"\"\"\n",
    "    # Generate two dataset\n",
    "    data1 = generate_random_mg_data(n_dimensions, data1_size)\n",
    "    data2 = generate_random_mg_data(n_dimensions, data2_size)\n",
    "    # Merge\n",
    "    data = np.concatenate((data1,data2))\n",
    "\n",
    "    # Shuffle\n",
    "    shuffle(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "np.random.seed(9001)\n",
    "# Generate a random data set with two multivate gaussian distribution\n",
    "data = generate_random_data_2md(n_dimensions,int(n_samples*data1_ratio),int(n_samples*data2_ratio))\n",
    "\n",
    "# Label Anomaly if the number of 1s is less than 7\n",
    "data_rowsum = np.sum(data,axis = 1)\n",
    "labels = data_rowsum < Anomaly_Threshold # Anomaly if total # 1s is less than the threshold\n",
    "labels = labels*1\n",
    "\n",
    "print(\"Percentage of Anomaly in the dataset: \" + str(np.sum(labels)/len(labels))) # Find percentage of anomaly in the dataset\n",
    "print(data[labels == 1][:5]) # Print the first 5 rows of anomaly data as examples\n",
    "\n",
    "if np.sum(labels)/len(labels) > 0.2:\n",
    "    print(\"Too much anomaly: start cleaning!\")\n",
    "    labels_remove = (labels==1) & (np.random.rand(n_samples) <= 0.6) # Remove around 60% of anomalies\n",
    "    print(str(sum(labels_remove)) +' Anomalies are going to be removed.')\n",
    "    data = data[~labels_remove] # Remove the selected data\n",
    "    labels = labels[~labels_remove] # Remove the corresponding labels\n",
    "    print(\"Percentage of Anomaly in the dataset after cleaning: \" + str(np.sum(labels)/len(labels))) # Find percentage of anomaly in the dataset\n",
    "\n",
    "\n",
    "# Save the data and labels\n",
    "np.save('data.npy',data)\n",
    "np.save('labels.npy',labels)\n",
    "print('Data and Labels have been saved!')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
