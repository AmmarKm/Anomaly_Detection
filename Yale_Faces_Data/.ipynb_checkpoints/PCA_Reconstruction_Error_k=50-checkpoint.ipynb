{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Introduction\n",
    "In this notebook, I experimennt the visualization of Ranking Metrics for the Anomaly Detection Model after PCA. The problem is set up as \"Unsupervised Anomaly Detection\". I split the dataset into the Validation and Testing Dataset. \n",
    "\n",
    "The anomaly in the datast is a figure with mustache.\n",
    "\n",
    "Data Source: http://vision.ucsd.edu/~iskwak/ExtYaleDatabase/ExtYaleB.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt  \n",
    "from PIL import Image\n",
    "from scipy.io import loadmat  \n",
    "from scipy import stats  \n",
    "from scipy.stats import multivariate_normal\n",
    "import re\n",
    "import glob\n",
    "from operator import itemgetter \n",
    "import random\n",
    "from random import shuffle\n",
    "from processing_functions import * # local to this notebook\n",
    "\n",
    "import os\n",
    "os.chdir('../')\n",
    "\n",
    "from support_functions import *\n",
    "from PCA_Functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the number of Principal Components to keep from the image\n",
    "n_components  = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574  images are above our threshold and thus removed from the list\n"
     ]
    }
   ],
   "source": [
    "# Define the images to be read and the corresponding labels\n",
    "label_1_folder = [9,21]\n",
    "target_folders = range(1,22)\n",
    "data_path = \"Yale_Faces_Data/CroppedYale/\"\n",
    "\n",
    "# We also need to reduce the size of the image for the convenience of computation\n",
    "reduce_height = 24\n",
    "reduce_width = 21\n",
    "\n",
    "# Read the images and reduce the size\n",
    "images,labels = read_images(data_path,target_folders,label_1_folder,reduce_height,reduce_width)\n",
    "\n",
    "# To evaluate the threshold of the dark pixels\n",
    "# dark_pixel_curve(images)\n",
    "\n",
    "imgs = images[:] # Create a copy\n",
    "# Eliminate the images and labels whose number of dark pixels are above the threshold\n",
    "# The threshold is determined based on the dark_pixel_curve() function above\n",
    "imgs,labels,remove_count = remove_dark_img(imgs,labels,180) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply PCA on the Normal Images for Dimension Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the number of components to be remained in the PCA Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_components = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the dimension of one image\n",
    "height, width = imgs[0].shape\n",
    "num_imgs = len(imgs)\n",
    "\n",
    "# reshape to a 2-D Matrix\n",
    "imgs_matrix = np.zeros((num_imgs,height*width)) # Initialize the matrix to store the entire image list\n",
    "# Iterate through each image, convert it into an array, and add to the imgs_matrix as a new row\n",
    "for i in range(0,len(imgs)):\n",
    "    imgs_matrix[i,:] = imgs[i].reshape(height*width)\n",
    "\n",
    "# Vectorize the labels list\n",
    "labels_vector = np.hstack(labels) # Easier to get multiple items from a vector than from a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconstruct the images with PCA\n",
    "- Compute PCA on only the normal\n",
    "- Plot eigenfaces\n",
    "- Encode and Decode the images with PCA\n",
    "- Plot the comparison between the original and reconstructed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8fe7bc4b6579>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimgs_reconstructed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpca_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponent_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca_all_processes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels_vector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplot_eigenfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplot_comparison\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ivan/ML/Anomaly_Detection/PCA_Functions.py\u001b[0m in \u001b[0;36mpca_all_processes\u001b[0;34m(data, labels, n_components, plot_eigenfaces, plot_comparison, height, width)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mplot_eigenfaces\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m# Visualize the eigenfaces with the pca matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mplot_eigenfaces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;31m# Encode and then decode the entire dataset with the pca_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'bool' object is not callable"
     ]
    }
   ],
   "source": [
    "imgs_reconstructed,pca_matrix, component_mean = pca_all_processes(imgs_matrix,labels_vector,n_components,plot_eigenfaces_bool = True,plot_comparison_bool = True,height=height,width=width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection with Reconstruction Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data into the Validation and Test Set\n",
    "With the function, we get the indice of images to be assigned to the Training and Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the images and labels\n",
    "ratio_train = 0.7 # No training set\n",
    "train_ind, test_ind = split_training(labels_vector,ratio_train)\n",
    "\n",
    "img_train = imgs_matrix[train_ind,:] # Original Image Set\n",
    "img_train_pca = imgs_reconstructed[train_ind,:] # Reconstructed Image Set\n",
    "\n",
    "img_test = imgs_matrix[test_ind,:] # Original Image Set\n",
    "img_test_pca = imgs_reconstructed[test_ind,:] # Reconstructed Image Set\n",
    "\n",
    "labels_train = labels_vector[train_ind]\n",
    "labels_test = labels_vector[test_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the Optimal Threshold based on the Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the euclidean distance between the reconstructed dataset and the original ()\n",
    "dist_train = find_euclidean_distance(img_train_pca,img_train)\n",
    "print(\"The higher the reconstruction error, the more likely the point will be an anomaly\")\n",
    "plot_scatter_with_labels(dist_train,labels_train,\"Reconstruction Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 20 # Define the k parameter for the precision at k\n",
    "threshold_error = select_threshold_distance(dist_train, labels_train,k,to_print = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Detector with the Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the euclidean distance between the reconstructed dataset and the original ()\n",
    "dist_test = find_euclidean_distance(img_test_pca,img_test)\n",
    "\n",
    "# Sort the Images and Labels based on the Probability\n",
    "rank_test = np.argsort(-dist_test) # Sort from the Largest to the Smallest\n",
    "dist_test_ranked = dist_test[rank_test] # Sort the distance\n",
    "# Rank Labels\n",
    "labels_test_ranked = labels_test[rank_test]\n",
    "# Give Predictions\n",
    "preds = np.zeros(labels_test.shape) # Initialization\n",
    "preds[dist_test_ranked > threshold_error] = 1\n",
    "\n",
    "# Evaluate the Detector with Testing Data\n",
    "eval_with_test(preds, labels_test_ranked, k = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
